import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime, timedelta
from prophet import Prophet
from prophet.serialize import model_from_json, model_to_json
from darts.models import TCNModel
from darts import TimeSeries
import torch
import tensorflow as tf
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow import keras
from keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector,  TimeDistributed, Attention, Concatenate, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import callbacks
import pandas as pd
import matplotlib.pyplot as plt
import darts
from darts import TimeSeries
from darts.models import TransformerModel
from darts.dataprocessing.transformers import Scaler
from sklearn.metrics import mean_absolute_error
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Time Series Forecasting",
    layout="wide",
    page_icon="üìà"
)


# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
@st.cache_resource
def load_pretrained_models():
    models = {}
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Prophet
        with open('prophet_model.json', 'r') as f:
            models['prophet'] = model_from_json(f.read())

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ TCN
        models['tcn'] = TCNModel.load("darts_future_model.pt")

        # –ó–∞–≥—Ä—É–∑–∫–∞ TensorFlow –º–æ–¥–µ–ª–∏
        models['tf'] = tf.keras.models.load_model('tf_model.keras')

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}")
    return models


def validate_dataset(df):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"""
    if (df['y'] < 0).any():
        negative_rows = df[df['y'] < 0]
        return False, negative_rows
    return True, None


def load_default_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    df = pd.read_csv('SunTable.csv', index_col=False)
    df.drop(df[df.isnull().N == True].index, inplace=True)
    df['days'] = df.apply(lambda x: x.N // 24, axis=1)
    df['months'] = df.apply(lambda x: x.N // (30 * 24), axis=1)
    df['years'] = df.apply(lambda x: x.N // (365 * 24), axis=1)
    df.drop('N', axis=1, inplace=True)

    DataDays = df.groupby('days').agg(
        {'V': 'mean', 'T': 'mean', 'P': 'mean', 'DirectRad': 'sum', 'ScatterRad': 'sum', 'SumRad': 'sum'})
    DataMonths = df.groupby('months').agg(
        {'V': 'mean', 'T': 'mean', 'P': 'mean', 'DirectRad': 'sum', 'ScatterRad': 'sum', 'SumRad': 'sum'})
    DataYears = df.groupby('years').agg(
        {'V': 'mean', 'T': 'mean', 'P': 'mean', 'DirectRad': 'sum', 'ScatterRad': 'sum', 'SumRad': 'sum'})

    DataDays = DataDays.reset_index()
    start_date = "2014-01-01"
    X_Prophet = DataDays[['days', 'SumRad']]

    X_Prophet['days'] = pd.to_datetime(start_date) + pd.to_timedelta(X_Prophet["days"], unit="D")
    X_Prophet.rename(columns={"days": "ds", "SumRad": "y"}, inplace=True)
    df1 = X_Prophet
    df1 = df1[:-1]
    return df1


def prepare_data(df, days=365):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
    df = df.sort_values('ds').tail(days)
    return df.set_index('ds')


def train_models(train_data):
    """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    models = {}

    # Prophet
    modelP = Prophet(
        yearly_seasonality=True,  # –í–∫–ª—é—á–∏—Ç—å –≥–æ–¥–æ–≤—É—é —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
        weekly_seasonality=False,  # –û—Ç–∫–ª—é—á–∏—Ç—å, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–µ–¥–µ–ª—å–Ω—ã–µ
        daily_seasonality=False,
        seasonality_mode='multiplicative',  # –î–ª—è —Ä–∞—Å—Ç—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤
        changepoint_prior_scale=0.05  # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Ä–µ–∑–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ç—Ä–µ–Ω–¥–∞
    )

    prophet_model = modelP
    prophet_model.fit(train_data)
    models['prophet'] = prophet_model

    # TCN
    series = TimeSeries.from_dataframe(train_data, 'ds', 'y')

    train_series, test_series = series.split_before(pd.Timestamp('2024-12-24'))
    scaler = Scaler()
    train_scaled = scaler.fit_transform(train_series)
    test_scaled = scaler.transform(test_series)

    model = TransformerModel(
        input_chunk_length=365,
        output_chunk_length=365,
        nhead=4,
        num_encoder_layers=2,
        num_decoder_layers=2,
        d_model=64,
        batch_size=32,
        pl_trainer_kwargs={"accelerator": "cpu"}  # –î–ª—è GPU –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ "gpu"
    )

    model.fit(train_scaled, epochs=20, verbose=True)
    models['tcn'] = model

    # tensorflow
    scaler = MinMaxScaler(feature_range=(0, 1))
    train_data['y'] = scaler.fit_transform(train_data[['y']])

    def create_dataset_from_df(dataframe, window_size, forecast_horizon):
        X, y = [], []
        data = dataframe['y'].values  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–∞, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–≤—è–∑—å —Å –∏—Å—Ö–æ–¥–Ω—ã–º DataFrame
        for i in range(len(data) - window_size - forecast_horizon + 1):
            X.append(data[i:i + window_size])

            # –ë–µ—Ä–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ DataFrame –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞—Ç
            start_idx = i + window_size
            end_idx = i + window_size + forecast_horizon
            y.append(data[start_idx:end_idx])

            # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–Ω–µ–π
            # if dataframe['ds'].iloc[end_idx-1] - dataframe['ds'].iloc[start_idx] != forecast_horizon-1:
            # print(f"–û—à–∏–±–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–∞—Ö: {i}")

        return np.array(X), np.array(y)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    window_size = 730  # 2 –≥–æ–¥–∞ –∏—Å—Ç–æ—Ä–∏–∏
    forecast_horizon = 365  # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –≥–æ–¥

    X, y = create_dataset_from_df(train_data, window_size, forecast_horizon)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ 3D-–º–∞—Å—Å–∏–≤ (samples, timesteps, features)
    X = X.reshape(-1, window_size, 1)
    y = y.reshape(-1, forecast_horizon, 1)

    def build_model(window_size, forecast_horizon):
        # –≠–Ω–∫–æ–¥–µ—Ä
        encoder_inputs = Input(shape=(window_size, 1))
        encoder = LSTM(128, return_sequences=False, dropout=0.2)(encoder_inputs)

        # –î–µ–∫–æ–¥–µ—Ä
        decoder_input = RepeatVector(forecast_horizon)(encoder)
        decoder = LSTM(64, return_sequences=True, dropout=0.2)(decoder_input)
        decoder_output = TimeDistributed(Dense(1))(decoder)  # –õ–∏–Ω–µ–π–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è

        model = Model(encoder_inputs, decoder_output)
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')
        return model

    model = build_model(window_size, forecast_horizon)
    model.summary()

    split_idx = int(0.9 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    early_stop = callbacks.EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True
    )

    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=20,
        batch_size=64,
        callbacks=[
            callbacks.EarlyStopping(monitor='val_loss', patience=15),
            callbacks.ReduceLROnPlateau(factor=0.5, patience=7)
        ],
        verbose=1
    )

    models['tf'] = model

    return models


def make_predictions(models, data, model_type):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""
    try:
        if model_type == 'prophet':
            future = models['prophet'].make_future_dataframe(periods=365)
            forecast = models['prophet'].predict(future)
            return forecast[['ds', 'yhat']].rename(columns={'yhat': 'y'}).set_index('ds').tail(365)

        elif model_type == 'tcn':

            series = TimeSeries.from_dataframe(data, 'ds', 'y')

            scaler = Scaler()
            train_scaled = scaler.fit_transform(series)

            pred_scaled = models['tcn'].predict(n=365)
            pred = scaler.inverse_transform(pred_scaled)

            dates = pred.time_index
            values = pred.values()

            pred_df = pd.DataFrame({"ds": dates, "y": values.flatten()})
            pred_df = pred_df.set_index('ds')
            return pred_df


        elif model_type == 'tf':
            scaler = MinMaxScaler(feature_range=(0, 1))

            last_sequence = data[['y']].values[-730:]
            last_sequence = scaler.fit_transform(last_sequence)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏: (1 –ø—Ä–∏–º–µ—Ä, 730 –¥–Ω–µ–π, 1 –ø—Ä–∏–∑–Ω–∞–∫)
            input_seq = last_sequence.reshape(1, 730, 1)

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 365 –¥–Ω–µ–π
            predictions = models['tf'].predict(input_seq, verbose=0)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞—Ç—ã —Å 2025-12-30 –ø–æ 2026-12-29 (—Ä–æ–≤–Ω–æ 365 –¥–Ω–µ–π)
            forecast_dates = pd.date_range(
                start='2025-12-30',
                periods=365,
                freq='D'
            )

            prediction_actual = scaler.inverse_transform(predictions.reshape(-1, 1))

            # –°–æ–∑–¥–∞—ë–º DataFrame —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏
            return pd.DataFrame({'ds': forecast_dates, 'y': prediction_actual.flatten()}).set_index('ds')

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
    return None


def main():
    st.title("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤")
    models = load_pretrained_models()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    use_custom_data = st.sidebar.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ")

    if use_custom_data:
        uploaded_file = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Ñ–∞–π–ª", type=['csv'])
        if uploaded_file:
            try:
                df = pd.read_csv(uploaded_file)
                df['ds'] = pd.to_datetime(df['ds'])
                valid, error_data = validate_dataset(df)

                if not valid:
                    st.error("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö:")
                    st.write(error_data)
                    return

                data = prepare_data(df)
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
                return
    else:
        data = load_default_data()

    # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
    st.sidebar.header("–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã")
    mode = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", ['–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ', '–û–±—É—á–µ–Ω–∏–µ'])

    if mode == '–û–±—É—á–µ–Ω–∏–µ' and not use_custom_data:
        st.warning("–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ")
        return

    if mode == '–û–±—É—á–µ–Ω–∏–µ':
        st.header("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        if st.button("–ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ"):
            with st.spinner('–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...'):
                try:
                    models = train_models(data.reset_index())
                    st.success("–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω—ã!")

                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
                    with open('prophet_model.json', 'w') as f:
                        f.write(model_to_json(models['prophet']))

                    models['tcn'].save('darts_model.pkl')
                    models['tf'].save('tf_model.keras')

                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")

    st.header("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ")
    model_type = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", ['prophet', 'tcn', 'tf'])

    if st.button("–°–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑"):

        with st.spinner('–ò–¥–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ...'):
            forecast = make_predictions(models, data, model_type)

            if forecast is not None:
                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∞")
                data = data.set_index('ds')
                data = data.sort_values('ds').tail(365)
                print(data.columns)
                fig, ax = plt.subplots(figsize=(12, 6))
                print(data)
                print(forecast)
                data['y'].plot(ax=ax, label='–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ')
                forecast['y'].plot(ax=ax, color='red', label='–ü—Ä–æ–≥–Ω–æ–∑')
                plt.legend()
                st.pyplot(fig)

                st.download_button(
                    label="–°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑",
                    data=forecast.reset_index().to_csv(index=False),
                    file_name='forecast.csv'
                )


if __name__ == "__main__":
    main()